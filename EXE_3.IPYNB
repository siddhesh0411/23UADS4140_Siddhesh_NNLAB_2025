{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EXPERIMENT 3\n",
    "\n",
    "OBJECTIVE : WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.\n",
    "\n",
    "DESCRIPTION OF MODEL Model is a fully connected (feedforward) neural network with three layers, designed to classify handwritten digits (0-9) from the MNIST dataset.\n",
    "\n",
    "Model architecture\n",
    "\n",
    "Input layer : Each MNIST image is 28 × 28 pixels (784 pixels)--> 784 input size\n",
    "Hidden Layer 1 : 128 neurons , Activation Function: Sigmoid\n",
    "Hidden Layer 2 : 64 neurons ,Activation Function: Sigmoid\n",
    "Output Layer: 10 neurons (For each digit from 0 to 9) , No activation because it will be handled by softmax in the loss function.\n",
    "Model parameters(hyperparameters)\n",
    "\n",
    "Number of epochs : 10\n",
    "Learning rate : 0.01\n",
    "Batch size : 100\n",
    "Loss function : Softmax Cross-Entropy\n",
    "Optimiser : Adam\n",
    "-Model follows a structured approach using forward propagation, loss calculation, backpropagation, and optimization to learn the correct classification.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5126, Training Accuracy: 0.9325\n",
      "Epoch 2, Loss: 0.2047, Training Accuracy: 0.9517\n",
      "Epoch 3, Loss: 0.1539, Training Accuracy: 0.9623\n",
      "Epoch 4, Loss: 0.1302, Training Accuracy: 0.9700\n",
      "Epoch 5, Loss: 0.1038, Training Accuracy: 0.9747\n",
      "Epoch 6, Loss: 0.0936, Training Accuracy: 0.9793\n",
      "Epoch 7, Loss: 0.0838, Training Accuracy: 0.9761\n",
      "Epoch 8, Loss: 0.0776, Training Accuracy: 0.9778\n",
      "Epoch 9, Loss: 0.0708, Training Accuracy: 0.9835\n",
      "Epoch 10, Loss: 0.0671, Training Accuracy: 0.9812\n",
      "Test Accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "#PYTHON IMPLEMENTATION\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    image = tf.reshape(image, [-1])  # Flatten to 784\n",
    "    label = tf.one_hot(label, depth=10)  # Convert to one-hot encoding\n",
    "    return image, label\n",
    "\n",
    "# Load dataset and apply preprocessing\n",
    "mnist_dataset = tfds.load(\"mnist\", split=[\"train\", \"test\"], as_supervised=True)\n",
    "train_data = mnist_dataset[0].map(preprocess).shuffle(10000).batch(100)\n",
    "test_data = mnist_dataset[1].map(preprocess).batch(100)\n",
    "\n",
    "# Define neural network parameters\n",
    "input_size = 784\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_size, hidden_layer1_size]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([hidden_layer1_size, hidden_layer2_size]))\n",
    "b2 = tf.Variable(tf.zeros([hidden_layer2_size]))\n",
    "\n",
    "W_out = tf.Variable(tf.random.normal([hidden_layer2_size, output_size]))\n",
    "b_out = tf.Variable(tf.zeros([output_size]))\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(X):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    output_layer = tf.matmul(layer2, W_out) + b_out  # No activation (logits)\n",
    "    return output_layer\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Training step function\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward_pass(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W_out, b_out])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W_out, b_out]))\n",
    "    return loss\n",
    "\n",
    "# Compute accuracy\n",
    "\n",
    "\n",
    "def compute_accuracy(dataset):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for X, Y in dataset:\n",
    "        logits = forward_pass(X)\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "        total_correct += tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "        total_samples += X.shape[0]\n",
    "    return total_correct / total_samples\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_x, batch_y in train_data:\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "        avg_loss += loss\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss /= total_batches\n",
    "    train_acc = compute_accuracy(train_data)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_acc = compute_accuracy(test_data)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DESCRIPTION OF CODE\n",
    "\n",
    "We are using TensorFlow . TensorFlow is an open-source machine learning library developed by Google. It is used to build and train AI models, especially deep learning models like neural networks.\n",
    "Imported tensorflow , tensorflow_datasets : Helps in loading prebuilt datasets like MNIST.\n",
    "def preprocess() -->\n",
    "\n",
    "Dataset is loaded and preprocessing is done . Pixel values are normalised between [0,1] .\n",
    "Dataset is converted to a 1D array .\n",
    "One hot encoding is done to convert labels to binary format.\n",
    "mnist dataset is split into train and test dataset each containing tuple (image , label) , both are TensorFlow dataset objects.\n",
    "map(preprocess) : Applies preprocessing to every image in the dataset.\n",
    "shuffle(10000) : Randomly shuffles 10,000 images to improve model generalization.\n",
    "batch(100) : Divides dataset into mini-batches of 100 for training. For 100 forward passes , backward propagation will be performed once.\n",
    "Neural network parameters are defined :\n",
    "\n",
    "input_size = 784\n",
    "hidden_layer1_size = 128\n",
    "hidden_layer2_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "Weight and bias initialisation :\n",
    "\n",
    "Weights (W1, W2, W_out) : Randomly initialized\n",
    "Biases (b1, b2, b_out) : Initialized as zero\n",
    "def forward_pass() -->\n",
    "\n",
    "Layer 1 output : A1 = sigmoid(XW1 + b1)\n",
    "Layer 2 output : A2 = sigmoid(A1W2 + b2)\n",
    "Output Layer (Logits): Output(Z) = (A2W_out + b_out)\n",
    "No activation in the last layer because it will be handled by softmax in the loss function.\n",
    "def loss_fn() -->\n",
    "\n",
    "Softmax Cross-Entropy Loss: Measures how different the predictions are from true labels.\n",
    "Loss = −∑ (Yi) log(softmax(Zi))\n",
    "tf.reduce_mean(): Computes average loss across all batches.\n",
    "Adam Optimizer : Optimizer adjusts the weights and biases to minimize the loss function during training.\n",
    "\n",
    "def train_step() -->\n",
    "\n",
    "tf.GradientTape(): Computes gradients automatically.\n",
    "optimizer.apply_gradients(): Updates weights using the calculated gradients.\n",
    "def compute_accuracy() -->\n",
    "\n",
    "Predictions (argmax): Returns the index of the highest probability class.\n",
    "Correct Predictions (equal()): Compares predictions with true labels.\n",
    "Function helps compute accuracy : correct predictions/total predictions .\n",
    "Training loop :\n",
    "\n",
    "Loops over 10 epochs.\n",
    "Calls train_step() on each batch.\n",
    "Computes training loss and accuracy.\n",
    "Finally the test accuracy has been calculated which comes out to be 96.55% .\n",
    "\n",
    "MY COMMENTS(limitations and scope of improvement)\n",
    "\n",
    "The model has been trained to classify the mnist dataset and achived a training accuracy of 98.44% and final test accuracy of 96.55%.\n",
    "Accuracy could be improved by using an activation function other than Sigmoid as it is prone to vanishing gardient. ReLU activation function could have been used.\n",
    "Accuracy could have been improved by increasing the number of epochs , changing the number of hidden neurons , using other optimiser or another loss function.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
