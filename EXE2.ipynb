{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 2:\n",
    "\"\"\" WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python.\n",
    "Demonstrate that it can learn the XOR Boolean function.  \"\"\"\n",
    "\n",
    "#Objective\n",
    "\"\"\"The objective of this program is to implement a Multi-Layer Perceptron (MLP) with a single \n",
    "hidden layer using NumPy and train it to learn the XOR Boolean function. \n",
    "The XOR function is not linearly separable, requiring at least one hidden layer for correct classification.\"\"\"\n",
    "#model discription\n",
    "\"\"\"\n",
    "Description of the Model\n",
    "The model is a two-layer perceptron with:\n",
    "Input Layer: 2 neurons (one for each input feature).\n",
    "Hidden Layer: 2 neurons with a step function activation.\n",
    "Output Layer: 1 neuron with a step function activation.\n",
    "The model is trained using backpropagation to minimize the error between predicted and actual outputs.\n",
    "The step function is used as the activation function, which makes backpropagation ineffective due to zero gradients. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
    "        self.weights = np.random.randn(input_size + 1)  \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  \n",
    "        return self.activation(np.dot(self.weights, x))\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                y_pred = self.activation(np.dot(self.weights, X[i]))\n",
    "                if y[i] == 1 and y_pred == 0:\n",
    "                    self.weights += self.learning_rate * X[i]\n",
    "                elif y[i] == 0 and y_pred == 1:\n",
    "                    self.weights -= self.learning_rate * X[i]\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = [self.predict(x) for x in X]\n",
    "        count = 0\n",
    "        for i in range(len(y)) :\n",
    "            if y_pred[i]==y[i] : count+=1\n",
    "        accuracy = count / len(y)\n",
    "        return accuracy, y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final Perceptron Weights: [ 0.51913633 -0.64425205 -0.50167162  0.33876224 -0.81714374]\n",
      "final Perceptron Predictions: [0, 1, 1, 0]\n",
      "final Perceptron Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "fun1_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun1_y = np.array([0, 0, 0, 1])  \n",
    "\n",
    "hiddenPerceptron1 = Perceptron(input_size=2)\n",
    "hiddenPerceptron1.train(fun1_X, fun1_y)\n",
    "fun1_accuracy, predictionsLayer1 = hiddenPerceptron1.evaluate(fun1_X, fun1_y)\n",
    "fun2_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun2_y = np.array([0, 0, 1, 0])\n",
    "\n",
    "hiddenPerceptron2 = Perceptron(input_size=2)\n",
    "hiddenPerceptron2.train(fun2_X, fun2_y)\n",
    "fun2_accuracy, predictionsLayer2 = hiddenPerceptron2.evaluate(fun2_X, fun2_y)\n",
    "fun3_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun3_y = np.array([0, 1, 0, 0])  \n",
    "\n",
    "hiddenPerceptron3 = Perceptron(input_size=2)\n",
    "hiddenPerceptron3.train(fun3_X, fun3_y)\n",
    "fun3_accuracy, predictionsLayer3 = hiddenPerceptron3.evaluate(fun3_X, fun3_y)\n",
    "fun4_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "fun4_y = np.array([1, 0, 0, 0])  \n",
    "\n",
    "hiddenPerceptron4 = Perceptron(input_size=2)\n",
    "hiddenPerceptron4.train(fun4_X, fun4_y)\n",
    "fun4_accuracy, predictionsLayer4 = hiddenPerceptron4.evaluate(fun4_X, fun4_y)\n",
    "X = np.array([predictionsLayer1, predictionsLayer2, predictionsLayer3, predictionsLayer4])\n",
    "y = np.array ([0,1,1,0]) \n",
    "perceptron = Perceptron(input_size=4)\n",
    "perceptron.train(X, y)\n",
    "accuracy, final_predictions = perceptron.evaluate(X, y)\n",
    "print(f\"\\nfinal Perceptron Weights: {perceptron.weights}\")\n",
    "print(f\"final Perceptron Predictions: {final_predictions}\")\n",
    "print(f\"final Perceptron Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription of the Code:\\nStep Function and Its Derivative:\\n\\nstep_function(x): Outputs 1 for values ≥ 0, otherwise 0.\\nstep_derivative(x): Returns 0 because the step function is not differentiable.\\nData Preparation:\\n\\nXOR truth table is stored in X (inputs) and y (expected outputs).\\nWeight and Bias Initialization:\\n\\nRandomly initialized for both hidden and output layers.\\nTraining Loop:\\n\\nForward Pass: Computes activations for hidden and output layers.\\nError Calculation: Finds the difference between predicted and actual values.\\nBackpropagation: Attempts weight updates (but is ineffective due to step function’s zero gradient).\\nWeight Update: Weights and biases are adjusted based on computed gradients.\\nTesting:\\n\\nOnce training is complete, predictions are printed for all XOR inputs.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of the Code:\n",
    "Step Function and Its Derivative:\n",
    "\n",
    "step_function(x): Outputs 1 for values ≥ 0, otherwise 0.\n",
    "step_derivative(x): Returns 0 because the step function is not differentiable.\n",
    "Data Preparation:\n",
    "\n",
    "XOR truth table is stored in X (inputs) and y (expected outputs).\n",
    "Weight and Bias Initialization:\n",
    "\n",
    "Randomly initialized for both hidden and output layers.\n",
    "Training Loop:\n",
    "\n",
    "Forward Pass: Computes activations for hidden and output layers.\n",
    "Error Calculation: Finds the difference between predicted and actual values.\n",
    "Backpropagation: Attempts weight updates (but is ineffective due to step function’s zero gradient).\n",
    "Weight Update: Weights and biases are adjusted based on computed gradients.\n",
    "Testing:\n",
    "\n",
    "Once training is complete, predictions are printed for all XOR inputs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"Performance Evaluation\n",
    "Expected Output for XOR Function:\n",
    "Input: [0 0] -> Output: 0  \n",
    "Input: [0 1] -> Output: 1  \n",
    "Input: [1 0] -> Output: 1  \n",
    "Input: [1 1] -> Output: 0  \n",
    "Actual Output:\n",
    "Due to the use of the step function, the model does not learn properly because weight updates do not happen effectively.\n",
    "The loss value remains constant during training, indicating no learning progress.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Comments\n",
    "Replace step function with ReLU or sigmoid for proper learning.\n",
    "Adjust learning rate and training strategy for better performance.\n",
    "Current approach demonstrates why differentiable activations are essential in MLPs.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
